[
  {
    "requirement": "General description of the AI system",
    "requirement_id": "req1",
    "coverage_score": 0.95,
    "matched_terms": [
      "AI System",
      "AI Provider",
      "intended purpose",
      "version",
      "AI Deployer",
      "is deployed by",
      "is provided by"
    ],
    "reasoning": "The ontology directly represents 'AI System', 'AI Provider', and 'AI Deployer' explicitly. Crucially, it includes 'intended purpose' and 'version' as attributes of the AI System, directly addressing the key components of the requirement. The 'is provided by' and 'is deployed by' relationships accurately capture the provider/deployer link. The coverage is very high due to the direct and specific nature of these matching terms.",
    "missing": [
      "Relation between AI System version and prior versions",
      "Information about the AI System’s context of use"
    ]
  },
  {
    "requirement": "Interaction with hardware/software",
    "requirement_id": "req2",
    "coverage_score": 0.85,
    "matched_terms": [
      "AI System",
      "Hardware Component",
      "Software Component",
      "Software Dependency",
      "Interface",
      "AI Deployer",
      "AI Provider",
      "depends on",
      "has component",
      "has software component",
      "has interface",
      "is deployed by",
      "is provided by",
      "AI System Capability",
      "AI Task"
    ],
    "reasoning": "The ontology provides granular representation of AI systems, their components (hardware and software), and the actors involved (deployer, provider). The relationships 'depends on', 'has component' and 'has software component' explicitly cover interactions with external systems. Concepts like 'Interface' and 'AI System Capability/Task' demonstrate how external components are engaged by the AI system itself. The high score reflects that most of the requirement’s core concepts are addressed; however, the dynamic nature of interaction *during* runtime is less explicitly modeled.",
    "missing": [
      "Runtime Interaction",
      "Inter-AI System Communication Protocol",
      "External System Dependency Profile",
      "Interaction Constraint",
      "Communication Interface Definition"
    ]
  },
  {
    "requirement": "Software/firmware versioning",
    "requirement_id": "req3",
    "coverage_score": 0.9,
    "matched_terms": [
      "AI System",
      "Software Component",
      "Software Implementation",
      "Software Dependency",
      "Model Versioning",
      "Change Log",
      "Data Sheet",
      "Training Data Sheet"
    ],
    "reasoning": "The ontology directly addresses the requirement through terms like 'AI System', 'Software Component', 'Software Implementation' and 'Model Versioning' which capture the components subject to versioning.  The 'Version' property itself is a key element. Furthermore, 'Change Log' and 'Data Sheet' offer mechanism for tracking versions and updates ensuring traceability. The ontology provides robust support for both tracking software and model versions.",
    "missing": [
      "Update Mechanism",
      "Update Policy",
      "Supported Versions",
      "End-of-Life (EOL) Version",
      "Compatibility Information"
    ]
  },
  {
    "requirement": "Deployment forms",
    "requirement_id": "req4",
    "coverage_score": 0.85,
    "matched_terms": [
      "AI System",
      "Software Component",
      "Hardware Component",
      "Interface",
      "Deployment",
      "Visual Documentation",
      "Software Implementation",
      "AI Deployer",
      "AI Provider"
    ],
    "reasoning": "The ontology provides strong coverage as it explicitly models AI Systems, their deployment, software and hardware components, interfaces, and the roles of deployers and providers. The 'Modality' property and 'Visual Documentation' both directly relate to how the AI system is presented or accessed, addressing the 'forms' aspect of the requirement.  While not a single property explicitly captures *all* aspects, combining these terms effectively represents the variety of deployment forms.",
    "missing": [
      "Packaging Format",
      "API Specification",
      "Deployment Environment",
      "Platform Dependency"
    ]
  },
  {
    "requirement": "Hardware required",
    "requirement_id": "req5",
    "coverage_score": 0.95,
    "matched_terms": [
      "Hardware Component",
      "Computational Resource",
      "resource type",
      "resource vendor",
      "resource configuration",
      "AI System",
      "intended purpose"
    ],
    "reasoning": "The ontology provides a good representation of the hardware requirement. 'Hardware Component' and 'Computational Resource', combined with the detailed resource properties (type, vendor, configuration), directly address the need to describe the hardware. 'AI System' is essential as the hardware supports it; 'intended purpose' is relevant as intended use cases influence hardware requirements. The coverage is high as the ontology allows for specifying detailed hardware specifications.",
    "missing": [
      "Supported Operating Systems",
      "Minimum Hardware Specifications",
      "Hardware Compatibility",
      "Hardware Security features"
    ]
  },
  {
    "requirement": "Illustrations and markings",
    "requirement_id": "req6",
    "coverage_score": 0.95,
    "matched_terms": [
      "AI System",
      "Visual Documentation",
      "has visual documentation",
      "intended purpose",
      "modality",
      "depicts"
    ],
    "reasoning": "The ontology directly covers the requirement through 'AI System' and 'Visual Documentation' linked by 'has visual documentation'.  The 'depicts' property and 'intended purpose' & 'modality' provide descriptive properties relevant to understanding *what* is documented and *how* the system is presented for conformity. The combination sufficiently represents the need to document physical characteristics for AI systems as required by the AI Act.",
    "missing": [
      "Product Component",
      "Marking Specification",
      "Illustration Specification"
    ]
  },
  {
    "requirement": "User interface and instructions for use",
    "requirement_id": "req7",
    "coverage_score": 0.85,
    "matched_terms": [
      "AI System",
      "AI Deployer",
      "Interface",
      "Visual Documentation",
      "intended purpose",
      "modality",
      "version",
      "has interface",
      "has visual documentation"
    ],
    "reasoning": "The ontology adequately represents the requirement for describing the user interface for both the deployer and the system itself.  Terms like 'Interface' and 'Visual Documentation' directly address the UI description aspect, and 'AI Deployer' identifies the target audience.  The inclusion of 'intended purpose', 'modality', and 'version' also contributes as understanding the context and specifics of the system is crucial for a useful interface description. However, the requirement explicitly mentions 'instructions for use,' which isn't directly captured.",
    "missing": [
      "Instructions for Use",
      "Deployment Instructions",
      "UI Documentation",
      "User Guide"
    ]
  },
  {
    "requirement": "Elements of system and development process",
    "requirement_id": "req8",
    "coverage_score": 0.85,
    "matched_terms": [
      "AI System",
      "AI Provider",
      "AI Model",
      "AI Method",
      "Software Component",
      "Software Implementation",
      "AI Activity",
      "Build and Integration Testing",
      "Data Training",
      "Model Engineering",
      "Model Packaging",
      "Software Code Pipeline",
      "Machine Learning Pipeline",
      "depends on",
      "has component",
      "has software component",
      "uses AI method",
      "intended purpose"
    ],
    "reasoning": "The ontology provides strong coverage of the development process, identifying the AI Provider, AI System, and its components (Models, Software). The focus on AI Activities like Build and Integration Testing, Data Training, and Model Engineering, coupled with properties 'depends on' and 'has component' effectively captures the relationships between different elements. The inclusion of pipelines and 'intended purpose' also contributes to a comprehensive understanding of the requirement. The lack of a single term explicitly representing 'steps performed' slightly lowers the score.",
    "missing": [
      "Development Process",
      "Integration Details",
      "Modification Details",
      "Provenance Details",
      "Tool Usage Details",
      "Pre-trained System Usage"
    ]
  },
  {
    "requirement": "Design specifications of the system",
    "requirement_id": "req9",
    "coverage_score": 0.85,
    "matched_terms": [
      "AI System",
      "AI Model",
      "AI Method",
      "AI System Capability",
      "AI Task",
      "Software Component",
      "Software Implementation",
      "Data Pipeline",
      "Data Training",
      "Data Validation",
      "Data Testing",
      "Dataset",
      "Training Data Sheet",
      "Labeling Procedure",
      "Data Cleaning Procedure",
      "intended purpose",
      "modality",
      "version",
      "Performance Metric",
      "Model Engineering",
      "Model Evaluation",
      "Software Dependency",
      "Hardware Component",
      "Change Log",
      "Visual Documentation",
      "depicts",
      "resource type",
      "resource vendor",
      "resource configuration",
      "data scope",
      "data characteristic",
      "data collection method",
      "data selection criteria",
      "annotator type",
      "annotation tool",
      "outlier handling method",
      "missing data strategy",
      "quality metrics used"
    ],
    "reasoning": "The ontology provides a comprehensive framework for describing AI systems, including their components, data pipelines, and development processes.  It covers most aspects of the AI Act requirement including design choices, optimization goals, expected output, and trade-offs. However, there's a slight gap in explicitly representing the ‘rationale and assumptions’ behind design choices and the specifics of trade-off decisions beyond merely acknowledging their existence.",
    "missing": [
      "Design Rationale",
      "Trade-off Analysis",
      "System Optimization Criteria (more specific than performance metric)",
      "Assumptions about Users",
      "Limitations of the System",
      "Risk Assessment Data (linkable to AIRO ontology)"
    ]
  },
  {
    "requirement": "System architecture",
    "requirement_id": "req10",
    "coverage_score": 0.85,
    "matched_terms": [
      "AI System",
      "Software Component",
      "Software Implementation",
      "Computational Resource",
      "AI Deployer",
      "AI Provider",
      "AI Activity",
      "Build and Integration Testing",
      "Software Code Pipeline",
      "Data Pipeline",
      "Machine Learning Pipeline",
      "has component",
      "has software component",
      "depends on",
      "feeds into component",
      "resource type",
      "resource vendor",
      "resource configuration",
      "has lifecycle stage"
    ],
    "reasoning": "The ontology provides strong coverage for describing the system architecture, software components, and computational resources used. Terms like 'AI System', 'Software Component', and 'Computational Resource' directly address the requirement. Relationships like 'has component' and 'feeds into component' capture the interactions between components. While it doesn't explicitly *require* an understanding of the AI lifecycle to fulfill this AI Act requirement, linking to 'AI Activity' and 'has lifecycle stage' provides helpful context for understanding where in the process these components are used.",
    "missing": [
      "System Architecture Description",
      "Integration Diagram",
      "Deployment Environment",
      "Development Environment",
      "Testing Environment"
    ]
  },
  {
    "requirement": "Data requirements",
    "requirement_id": "req11",
    "coverage_score": 0.95,
    "matched_terms": [
      "Data Sheet",
      "Training Data Sheet",
      "Dataset",
      "Data Acquisition Activity",
      "Data Training",
      "Data Wrangling/Cleaning",
      "Data Validation",
      "Data Testing",
      "Labeling Procedure",
      "Data Cleaning Procedure",
      "data scope",
      "data characteristic",
      "data collection method",
      "data selection criteria",
      "labeling guideline",
      "annotator type",
      "annotation tool",
      "outlier handling method",
      "missing data strategy",
      "quality metrics used"
    ],
    "reasoning": "The ontology provides excellent coverage for this requirement, specifically around data documentation and lifecycle activities. Concepts like 'Data Sheet', 'Training Data Sheet' and the various data activities (Acquisition, Training, Wrangling, Validation, Testing) directly address the need for describing training methodologies, datasets and cleaning procedures. The detailed data-related properties (scope, characteristics, collection method, selection criteria, cleaning procedures etc.) provide a comprehensive level of data description. The high score reflects that almost all aspects of the AI Act requirement are explicitly or implicitly represented within these ontology terms.",
    "missing": [
      "Data Provenance (though partially covered by the use of prov:Activity as a base)",
      "Explicit documentation of data biases – while the ontology allows linking to bias information via quality annotations, there’s no dedicated concept.",
      "Granular definitions of data formats and schemas – while 'Data Characteristic' touches on this, a dedicated concept reflecting data structure would be beneficial."
    ]
  },
  {
    "requirement": "Assessment of human oversight measures",
    "requirement_id": "req12",
    "coverage_score": 0.85,
    "matched_terms": [
      "AI System",
      "AI Deployer",
      "Human Oversight Mechanism",
      "Interface",
      "Transparency Measure",
      "Explainable AI Feature",
      "AI Task",
      "AI System Capability",
      "Visual Documentation",
      "Data Sheet",
      "Training Data Sheet",
      "Labeling Procedure",
      "Data Validation",
      "Data Testing"
    ],
    "reasoning": "The ontology comprehensively covers the requirement to assess human oversight measures and technical measures for interpreting AI outputs, particularly through 'Human Oversight Mechanism', 'Transparency Measure' and links to the 'AI Deployer'. The ability to connect an 'AI System', 'AI Task' and 'AI System Capability' allows linkage to the type of AI being deployed, and connection to 'Data Validation' and 'Data Testing' is available. Furthermore, the ontology also covers documentation requirements via 'Visual Documentation' and 'Data Sheet'. The score is high, but could be further improved by explicit modeling of interpretability features beyond transparency.",
    "missing": [
      "Interpretability Feature",
      "Interpretability Assessment Procedure",
      "Output Explanation Format",
      "Human-AI Collaboration Protocol"
    ]
  },
  {
    "requirement": "Pre-determined changes to the AI system",
    "requirement_id": "req13",
    "coverage_score": 0.85,
    "matched_terms": [
      "AI System",
      "Change Log",
      "Model Versioning",
      "Data Pipeline",
      "Deployment",
      "Post-market Monitoring Activity",
      "Post-market Performance Evaluation Activity",
      "Software Component",
      "Software Implementation",
      "Data Training",
      "Data Validation",
      "Data Testing",
      "Model Engineering",
      "Performance Metric",
      "frequency",
      "AI Deployer",
      "AI Provider",
      "has component",
      "intended purpose",
      "modality",
      "version",
      "Software Dependency"
    ],
    "reasoning": "The ontology provides a good representation of the requirement by including concepts for AI systems, their components, life cycle phases (training, validation, testing, deployment, and post-market monitoring), and the need for tracking changes (Change Log, Model Versioning). The properties relating to deployments and providers support the 'who' aspects of compliance. The level of granularity allows for a detailed mapping of pre-determined changes and relevant information.  The score isn’t 1.0 as further linkage between these terms, specifically related to *continuous* compliance, is weak.",
    "missing": [
      "Compliance Audit Trail",
      "Deviation Management",
      "Remediation Plan",
      "Technical Solution Documentation",
      "Continuous Integration/Continuous Delivery (CI/CD) Pipeline Integration"
    ]
  },
  {
    "requirement": "Validation and testing procedures",
    "requirement_id": "req14",
    "coverage_score": 0.85,
    "matched_terms": [
      "AI System",
      "AI Deployer",
      "AI Provider",
      "AI Activity",
      "Data Training",
      "Data Validation",
      "Data Testing",
      "Data Pipeline",
      "Model Evaluation",
      "Log",
      "Data Monitoring and Logging",
      "Declaration of Conformity",
      "Change Log",
      "Performance Metric",
      "Dataset",
      "AI Artifact",
      "Training Data Sheet",
      "intended purpose",
      "data characteristic",
      "data scope",
      "data collection method"
    ],
    "reasoning": "The ontology comprehensively covers most aspects of the requirement, including the need to document validation/testing procedures, data characteristics, metrics, and reports.  The linkage between AI System, AI Deployer/Provider and the various data and evaluation activities makes it possible to trace the process and ensure accountability. The inclusion of detailed data sheet properties allows for specific attribute documentation. The core aspects of monitoring, logging, and change tracking directly address the requirement for test logs and reports.",
    "missing": [
      "Discriminatory Impact Assessment",
      "Predetermined Changes Documentation",
      "Test Environment Configuration",
      "Reproducibility Information"
    ]
  },
  {
    "requirement": "Cybersecurity measures",
    "requirement_id": "req15",
    "coverage_score": 0.85,
    "matched_terms": [
      "AI System",
      "Software Component",
      "Software Implementation",
      "Data Pipeline",
      "Data Wrangling/Cleaning",
      "Data Monitoring and Logging",
      "Data Validation",
      "Data Testing",
      "Deployment",
      "Change Log",
      "Post-market Monitoring Activity",
      "Post-market Performance Evaluation Activity",
      "Auditor",
      "Human Oversight Mechanism",
      "Declaration of Conformity",
      "Software Dependency"
    ],
    "reasoning": "The ontology provides strong support for representing cybersecurity measures through its coverage of system components, data handling, monitoring, deployment activities, auditing, and change management. The concepts allow for modeling the software and data pipelines upon which cybersecurity relies, alongside post-market monitoring to identify and address vulnerabilities. The relatively high score reflects a broad coverage, though specific cybersecurity protocols or threat modeling elements are absent.",
    "missing": [
      "Vulnerability Assessment",
      "Penetration Testing",
      "Incident Response Plan",
      "Security Audit Log",
      "Encryption Method",
      "Authentication Method",
      "Authorization Control",
      "Threat Model",
      "Security Policy",
      "Cybersecurity Standard (e.g., ISO 27001)",
      "Security Risk Assessment",
      "Security Patch Management"
    ]
  },
  {
    "requirement": "Monitoring, functioning and control",
    "requirement_id": "req16",
    "coverage_score": 0,
    "matched_terms": [],
    "reasoning": "Error: Expecting ',' delimiter: line 28 column 532 (char 1120)",
    "missing": []
  },
  {
    "requirement": "Appropriateness of performance metrics",
    "requirement_id": "req17",
    "coverage_score": 0.85,
    "matched_terms": [
      "Performance Metric",
      "AI System",
      "AI System Capability",
      "AI Task",
      "Model Evaluation",
      "Data Training",
      "Data Validation",
      "Data Testing",
      "intended purpose",
      "evaluation method",
      "has performance metric"
    ],
    "reasoning": "The ontology adequately covers the requirement by explicitly defining 'Performance Metric' and linking it to evaluation activities.  It further connects AI systems to their capabilities and tasks, providing context for performance evaluation. The inclusion of data training/validation/testing as well as 'intended purpose' and linking these to evaluation methods further strengthen the coverage. The linkage between evaluation activity and performance metric is crucial.",
    "missing": [
      "Performance Metric Appropriateness",
      "Evaluation Context",
      "Acceptance Criteria",
      "Performance Trade-offs"
    ]
  },
  {
    "requirement": "Risk management",
    "requirement_id": "req18",
    "coverage_score": 0.85,
    "matched_terms": [
      "AI System",
      "AI Deployer",
      "AI Provider",
      "AI Activity",
      "Data Acquisition Activity",
      "Data Wrangling/Cleaning",
      "Data Pipeline",
      "Data Training",
      "Data Validation",
      "Data Testing",
      "Model Engineering",
      "Model Versioning",
      "Deployment",
      "Post-market Monitoring Activity",
      "Post-market Performance Evaluation Activity",
      "Change Log",
      "Declaration of Conformity",
      "Human Oversight Mechanism",
      "has component",
      "has software component",
      "is deployed by",
      "is provided by",
      "logs activity",
      "has lifecycle stage",
      "intended purpose",
      "frequency",
      "Labeling Procedure"
    ],
    "reasoning": "The ontology provides a strong foundation for representing the AI risk management system requirement. It covers key lifecycle stages, roles (deployer, provider), data activities, model engineering, deployment, and monitoring. The inclusion of risk modelling (using AIRO), logging, and change logs demonstrates an understanding of traceability and auditability which is critical for risk management. However, there is a lack of explicit modelling of *how* risks are identified, assessed, and mitigated, beyond simply *that* risks exist.",
    "missing": [
      "Risk Assessment Procedure: A defined process for identifying, analyzing, and evaluating risks.",
      "Risk Mitigation Plan: A documented plan outlining steps to reduce or eliminate identified risks.",
      "Risk Register: A central repository for tracking identified risks, their assessment, and mitigation plans.",
      "Risk Tolerance Level: The acceptable level of risk for the AI system.",
      "Incident Response Plan: A plan specifying actions to take in the event of an incident related to the AI system.",
      "Audit Trail: A detailed, time-stamped record of actions taken relating to risk management."
    ]
  },
  {
    "requirement": "Changes to the AI system throughout its lifecycle",
    "requirement_id": "req19",
    "coverage_score": 0.85,
    "matched_terms": [
      "Change Log",
      "AI System",
      "AI Activity",
      "Software Component",
      "Model Versioning",
      "Software Implementation",
      "AI Artifact",
      "has lifecycle stage",
      "version",
      "Data Pipeline"
    ],
    "reasoning": "The ontology provides strong coverage by linking AI systems to lifecycle stages and activities, enabling tracking of changes over time. Terms like 'Change Log', 'Model Versioning', and 'Software Implementation' directly address modification tracking. The ability to link to Software Components and lifecycle phases allows for a comprehensive view of the system's evolution, though a dedicated activity specifically for 'changes' isn't explicitly defined as a core element across all artifacts.",
    "missing": [
      "AI System Modification Activity",
      "Modification Request",
      "Change Impact Assessment"
    ]
  },
  {
    "requirement": "Harmonised standards and common specifications",
    "requirement_id": "req20",
    "coverage_score": 0.85,
    "matched_terms": [
      "AI System",
      "Standard",
      "Declaration of Conformity",
      "AI Provider",
      "AI Deployer",
      "Data Sheet",
      "Training Data Sheet",
      "AI Artifact",
      "Software Component",
      "Hardware Component",
      "Software Dependency",
      "intended purpose",
      "version",
      "modality",
      "has component",
      "has software component",
      "depends on",
      "applies standard",
      "has declaration of conformity",
      "is provided by",
      "is deployed by"
    ],
    "reasoning": "The ontology robustly captures the requirement to list standards used ('Standard', 'applies standard') or, if not, describe solutions ('AI System', 'AI Artifact', 'Data Sheet'), as well as links to provider/deployer responsibilities ('AI Provider', 'AI Deployer', 'is provided by', 'is deployed by').  Concepts like 'Software Component' and 'Hardware Component' help detail the implementation alongside standards, and properties like 'depends on' address dependencies. However, it lacks explicit representation of a 'harmonised standards list' itself – only the *application* of standards is modelled.",
    "missing": [
      "Harmonised Standard List",
      "Solution Description",
      "Technical Specification"
    ]
  },
  {
    "requirement": "EU declaration of conformity",
    "requirement_id": "req21",
    "coverage_score": 0.95,
    "matched_terms": [
      "Declaration of Conformity",
      "AI System",
      "has declaration of conformity"
    ],
    "reasoning": "The ontology has a direct representation of the 'Declaration of Conformity' as a class, and it’s explicitly linked to 'AI System' through the 'has declaration of conformity' property. This linkage directly addresses the requirement for a copy of the EU declaration of conformity, making it very well covered. The high score reflects this direct and clear mapping.",
    "missing": [
      "Declaration of Conformity Version",
      "Declaration of Conformity Provider",
      "Declaration of Conformity Date"
    ]
  },
  {
    "requirement": "Post-market monitoring system",
    "requirement_id": "req22",
    "coverage_score": 0.85,
    "matched_terms": [
      "Post-market Monitoring Activity",
      "Post-market Performance Evaluation Activity",
      "AI System",
      "AI Deployer",
      "AI Provider",
      "Data Monitoring and Logging",
      "Performance Metric",
      "Log",
      "Data Pipeline",
      "Change Log",
      "Data Cleaning Procedure"
    ],
    "reasoning": "The ontology has strong representations for post-market monitoring (Post-market Monitoring Activity, Post-market Performance Evaluation Activity) , data logging/monitoring, performance evaluation (Performance Metric), and system details (AI System, AI Deployer, AI Provider). It covers the core elements needed to describe a system for evaluating AI performance and tracking changes. The inclusion of Frequency and the procedures are also very relevant. However, it lacks explicit coverage of the *plan* aspect, detailing the actual scheduling and methodology within the monitoring plan itself.",
    "missing": [
      "Post-Market Monitoring Plan",
      "Risk Re-evaluation Schedule",
      "Incident Response Protocol",
      "Escalation Procedure",
      "Performance Degradation Threshold",
      "Data Drift Detection Methodology"
    ]
  }
]